# Shivram Sriramulu's Data Portfolio

Welcome to my GitHub repository! ðŸ‘‹ I am Shivram Sriramulu, a passionate data professional based in San Jose. This repository serves as a showcase of my skills, projects, and accomplishments in the field.

## Table of Contents
- [About Me](#about-me)
- [Skills](#skills)
 - [Projects](#projects)
 - [Education](#education) 
- [Contact](#contact)

## About Me

I am a San Jose-based data enthusiast with a strong background in Data Analysis and Engineering. I love exploring and extracting meaningful insights from data, and I am skilled in Python, SQL, Spark, and R.

## Skills

### Data Analysis
- Exploratory Data Analysis (EDA)
- Data Cleaning and Preprocessing
- Statistical Analysis
- Apache Spark

### Data Engineering
- Database Design and Management
- ETL (Extract, Transform, Load) Processes
- Airflow
- Kafka
- Snowflake
- DBT
- Fivetran
- Spark
    
### Programming
- Python
- SQL
- R
- C++

### Machine Learning Tools
- Pandas
- NumPy
- SQLAlchemy
- TensorFlow
- Scikit-Learn
- PyTorch
- NLTK

### Visualization
- Matplotlib
- Seaborn
- Tableau
- Grafana
- PowerBI
  
### Databases
- MySQL
- MongoDB
- Neo4j
- Elasticsearch
- PostgreSQL

### Cloud Platforms
- AWS
  - Redshift
  - Athena
  - EMR
  - Glue
- GCP
  - BigQuery
  
## Projects

### [Project 1: PySpark Coindesk API to MySQL ETL]

- **Description:** This project demonstrates a PySpark script for fetching data from the Coindesk API and loading it into a MySQL database. The script utilizes PySpark to create a DataFrame from the API response and then writes the data to a MySQL table.

- **Role:** Sole developer and maintainer.

- **Tools/Technologies:**
  - Apache Spark
  - MySQL
  - Python

- **Impact:** The project provides a simple example of using PySpark for ETL purposes, enabling the extraction and loading of cryptocurrency data from the Coindesk API into a MySQL database.

**GitHub Repository:** [Link to the GitHub repository](https://github.com/ShivramSriramulu/SPARK-ETL-API)

### [Project 2: Data Engineering with PySpark and Airflow]

- **Description:** This project focuses on building a robust data engineering pipeline leveraging PySpark and Apache Airflow. It encompasses data extraction, transformation, and loading (ETL) processes to enhance data processing efficiency and automate daily tasks. The dataset, comprising ride-sharing information, is processed and transformed using PySpark, with scheduled tasks managed by Apache Airflow.

- **Role:** As the primary contributor, I played a pivotal role in designing, implementing, and maintaining the entire ETL pipeline. This involved optimizing PySpark transformations, orchestrating workflows with Apache Airflow, and ensuring seamless data integration.

- **Tools/Technologies:** 
  - **PySpark:** Used for efficient data processing and transformation tasks.
  - **Apache Airflow:** Orchestrated and scheduled ETL tasks, providing flexibility and reliability.
  - **Docker:** Containerized the pipeline components for consistent deployment.
  - **Git/GitHub:** Version control for collaborative development and project management.

- **Impact:** The project significantly improved data processing efficiency, enabling timely insights and informed decision-making. The dataset, comprising ride-sharing information, was successfully processed and transformed using PySpark. Scheduled daily ETL jobs streamlined repetitive tasks, enhancing automation and reducing manual intervention. The adoption of containerization facilitated easy setup and deployment, contributing to a more efficient and maintainable data engineering workflow.

**Dataset:** The dataset includes ride-sharing information with details such as ride_id, rideable_type, start and end timestamps, station information, geographical coordinates, and member type.

**GitHub Repository:** [Link to the GitHub repository](https://github.com/ShivramSriramulu/Airflow-Spark)

### [Project 3: MLIB-Scala-Airflow_Spark]

- **Description:** This project orchestrates Spark jobs written in Python, Scala, and MLlib using Apache Airflow, all within a Dockerized environment. The DAG `sparking_flow` is designed to submit Spark jobs for linear regression using MLlib in Python and Scala, ensuring that data processing is handled efficiently and reliably on a daily schedule.

- **Role:** Creator and contributor.

- **Tools/Technologies:**
  - Apache Airflow
  - Apache Spark
  - MLlib (Machine Learning library for Spark)
  - Docker

- **Impact:** The project facilitates the automation of Spark jobs, including linear regression using MLlib, across different programming languages. It enhances data processing capabilities in a scalable and maintainable way.

**GitHub Repository:** [Link to the GitHub repository](https://github.com/ShivramSriramulu/MLIB-Scala-Airflow_Spark)


## Education

- **Master's in Data Analytics**
  - **San Jose State University**
  - **Year of Graduation: 2025**
  - **Relevant Coursework:**
    - Database Management Systems (DBMS)
    - Mathematics
    - Data Visualizations


## Contact

Feel free to reach out to me! I am open to collaboration and eager to discuss data-related opportunities.

- **Email:** [shivramsriamulu01@gmail.com](mailto:shivramsriamulu01@gmail.com)
- **LinkedIn:** [Shivram Sriramulu](https://www.linkedin.com/in/shivram-sriramulu-86a9b1264/)

Let's connect and explore the world of data together! ðŸš€
